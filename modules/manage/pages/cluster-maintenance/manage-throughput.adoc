= Manage Throughput
:description: Manage the throughput of Kafka traffic with configurable properties.
:page-categories: Management, Networking

Redpanda supports applying throughput throttling on both ingress and egress independently, and allows configuration at the broker and client levels. The purpose of this is to prevent unbounded network and disk usage of the broker by clients. Broker-wide limits apply to all clients connected to the broker and restrict total traffic on the broker. Client limits apply to a set of clients defined by their `client_id` and help prevent a set of clients from starving other clients using the same broker.

== Throughput throttling enforcement

NOTE: As of v24.2, Redpanda enforces all throughput limits per broker, including client throughput.  

Throughput limits are enforced by applying backpressure to clients. When a connection is in breach of the throughput limit, the throttler advises the client about the delay (throttle time) that would bring the rate back to the allowed level. Redpanda starts by adding a `throttle_time_ms` field to responses. If that isn't honored, delays are inserted on the connection's next read operation. The throttling delay may not exceed the limit set by xref:reference:tunable-properties.adoc#max_kafka_throttle_delay_ms[`max_kafka_throttle_delay_ms`].

== Broker-wide throughput limits

Broker-wide throughput limits account for all Kafka API traffic going into or out of the broker. The limit values represent the allowed rate of data in bytes per second passing through in each direction. Redpanda also provides administrators the ability to exclude clients from throughput throttling and to fine-tune which Kafka request types are subject to throttling limits.

=== Broker-wide throughput limit properties

The properties for broker-wide throughput quota balancing are configured at the cluster level, for all brokers in a cluster:

|===
| Property | Description

| xref:reference:cluster-properties.adoc#kafka_throughput_limit_node_in_bps[kafka_throughput_limit_node_in_bps]
| A broker's total throughput limit for ingress Kafka traffic.

| xref:reference:cluster-properties.adoc#kafka_throughput_limit_node_out_bps[kafka_throughput_limit_node_out_bps]
| A broker's total throughput limit for egress Kafka traffic.

| xref:reference:cluster-properties.adoc#kafka_throughput_control[kafka_throughput_control]
| List of clients for whom broker-wide limits do not apply

| xref:reference:cluster-properties.adoc#kafka_throughput_controlled_api_keys[kafka_throughput_controlled_api_keys]
| List of Kafka request types subject to broker-wide throughput limits; defaults to `produce` and `fetch`.

| xref:reference:tunable-properties.adoc#max_kafka_throttle_delay_ms[max_kafka_throttle_delay_ms]
| The maximum delay inserted in the data path of Kafka API requests to throttle them down. Configuring this to be less than the Kafka client timeout can ensure that the delay that's inserted won't be long enough to cause a client timeout by itself.

|===

[NOTE]
====
* By default, both `kafka_throughput_limit_node_in_bps` and `kafka_throughput_limit_node_out_bps` are disabled, and no throughput limits are applied. You must manually set them to enable throughput throttling.
====

== Client throughput limits

Redpanda provides configurable throughput quotas that apply to an individual client or a group of clients. You can apply a quota for an individual client based on an exact match with its `client_id`, or a group of clients based on IDs that start with a given prefix. 

As of v24.2, client throughput quotas are compatible with the https://cwiki.apache.org/confluence/display/KAFKA/KIP-546%3A+Add+Client+Quota+APIs+to+the+Admin+Client[Kafka API^] (AlterClientQuotas and DescribeClientQuotas), and are separate from quotas configured through cluster configuration in earlier Redpanda versions. The client throughput quotas no longer apply on a per-shard basis, and now limit the rates across a Redpanda broker's node. The quotas are neither shared nor balanced between brokers.

Redpanda supports the following Kafka API-based quota types on clients:

|===
| Quota type | Description

| producer_byte_rate
| Limit throughput of produce requests

| consumer_byte_rate
| Limit throughput of fetch requests

| controller_mutation_rate
| Limit rate of topic mutation requests, including create, add, and delete partition, in number of partitions per second

|===

You can also apply a default quota for all other client requests that don't have a specific quota configured based on the exact or prefix match on `client_id`. 

It is possible to create conflicting quotas if you configure the same quotas through both the Kafka API and a cluster configuration. Redpanda resolves these conflicts by following an order of preference in finding a matching quota for a request:

. Quota configured through the Kafka API for an exact match on `client_id`
. Quota configured through the Kafka API for a prefix match on `client_id`
. Quota configured through cluster configuration properties (xref:reference:cluster-properties.adoc#kafka_client_group_byte_rate_quota[`kafka_client_group_byte_rate_quota`], xref:reference:cluster-properties.adoc#kafka_client_group_fetch_byte_rate_quota[`kafka_client_group_fetch_byte_rate_quota`]) for a prefix match on `client_id`
. Default quota configured through the Kafka API on `client_id`
. Default quota configured through cluster configuration properties on `client_id`

Redpanda Data recommends <<migrate,migrating>> over from cluster configuration-managed quotas to managing the quotas through the Kafka API. You can recreate the configuration-based quotas through the Kafka API using `rpk`, and then remove the quota cluster configs.

=== Individual client throughput limit

You can view current throughput quotas set through the Kafka API by running the `rpk cluster quotas describe` command.

For example, to see the quotas for client ID `consumer-1`:

[,bash]
----
rpk cluster quotas describe --name client-id=consumer-1
----

To set a throughput quota for a single client, use the `rpk cluster quotas alter` command. 

[,bash]
----
rpk cluster quotas alter --add consumer_byte_rate=200000 --name client-id-prefix=consumer-
----

=== Group of clients throughput limit

Alternatively, you can view or configure throughput quotas for a group of clients based on a match on client ID prefix. The following example sets the `consumer_byte_rate` quota to a group of clients that have IDs starting with `consumer-`:

[,bash]
----
rpk cluster quotas alter --add consumer_byte_rate=200000 --name client-id-prefix=consumer-
----

NOTE: The client group that matches on `client-id-prefix` does not denote a Kafka consumer group. Instead, it is the set of clients fetching from a Redpanda broker that are configured by the property to be throughput limited.

=== Default client throughput limit

You can apply default throughput limits to clients. Redpanda applies the default limits if no quotas have been configured specific to a client ID or ID prefix. Defaults are only configurable based on the exact client ID.

The example below sets a produce quota of 1GBps through the Kafka API, which means a 1GBps limit across all produce requests sent to a single Redpanda broker. 

[,bash]
----
rpk cluster quotas alter --default client-id --add producer_byte_rate=1000000000
----

[[migrate]]
=== Migrate cluster configuration quotas to Kafka API-based quotas

. Use `rpk cluster config` to view current client quotas managed with cluster configuration:
+
[,bash]
----
rpk cluster config get kafka_client_group_byte_rate_quota

----
// TODO: Confirm how to retrieve for multiple quota types:
// kafka_client_group_fetch_byte_rate_quota
// target_quota_byte_rate
// target_fetch_quota_byte_rate
// kafka_admin_topic_api_rate
+
[,bash]
----
1000000000
----

. Use `rpk cluster quotas alter` to set the corresponding client throughput quotas based on the Kafka API:
+
[,bash]
----
rpk cluster quotas alter --add producer_byte_rate=100000000 --add consumer_byte_rate=200000 --add controller_mutation_rate=10 --name client-id-prefix=consumer-
----
// TODO: Check example values 

. Use `rpk cluster config` to remove the configuration-based quotas:
+
[,bash]
----
rpk cluster config
----
// TODO: rpk command to delete/unset quotas

=== Monitor client throughput

The following metrics are available on both the `/public_metrics` and `/metrics` endpoints to help provide insight into client throughput quota usage:

* Client quota throughput per rule and quota type
** `/public_metrics` - `redpanda_kafka_quotas_client_quota_throughput`
** `/metrics` - `vectorized_kafka_quotas_client_quota_throughput`
* Client quota throttling delay, in seconds, per rule and quota type
** `/public_metrics` - `redpanda_kafka_quotas_client_quota_throttle_time`
** `/metrics` - `vectorized_kafka_quotas_client_quota_throttle_time`

The `kafka_quotas` logger provides details at the trace level on client quota throttling:

[,bash]
----
TRACE 2024-06-14 15:36:05,240 [shard  2:main] kafka_quotas - quota_manager.cc:361 - request: ctx:{quota_type: produce_quota, client_id: {rpk}}, key:k_client_id{rpk}, value:{limit: {1111}, rule: kafka_client_default}, bytes: 1316, delay:184518451ns, capped_delay:184518451ns
TRACE 2024-06-14 15:36:05,240 [shard  2:main] kafka_quotas - connection_context.cc:605 - [127.0.0.1:51256] throttle request:{snc:0, client:184}, enforce:{snc:-365123762, client:-365123762}, key:0, request_size:1316
TRACE 2024-06-14 15:37:44,835 [shard  2:main] kafka_quotas - quota_manager.cc:361 - request: ctx:{quota_type: produce_quota, client_id: {rpk}}, key:k_client_id{rpk}, value:{limit: {1111}, rule: kafka_client_default}, bytes: 119, delay:0ns, capped_delay:0ns
TRACE 2024-06-14 15:37:59,195 [shard  2:main] kafka_quotas - quota_manager.cc:361 - request: ctx:{quota_type: produce_quota, client_id: {rpk}}, key:k_client_id{rpk}, value:{limit: {1111}, rule: kafka_client_default}, bytes: 1316, delay:184518451ns, capped_delay:184518451ns
TRACE 2024-06-14 15:37:59,195 [shard  2:main] kafka_quotas - connection_context.cc:605 - [127.0.0.1:58636] throttle request:{snc:0, client:184}, enforce:{snc:-14359, client:-14359}, key:0, request_size:1316
----

